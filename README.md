

ğŸ¦ Citi-Themed RAG Chat Application (Local LLM with Ollama)

A full-stack ChatGPT-style application that performs Retrieval-Augmented Generation (RAG) on:
	â€¢	ğŸ“„ Uploaded documents
	â€¢	ğŸŒ Documents ingested from URLs

The system runs fully locally using Ollama with the gemma3:latest model and provides persistent chat memory using Redis.

â¸»

ğŸš€ Project Overview

This project implements a ChatGPT-style interface with:
	â€¢	ğŸ”¹ Local LLM inference via Ollama
	â€¢	ğŸ”¹ FastAPI backend with Uvicorn
	â€¢	ğŸ”¹ Vector search using ChromaDB
	â€¢	ğŸ”¹ Embeddings via open-source embedding models
	â€¢	ğŸ”¹ Redis-based chat memory & prompt history
	â€¢	ğŸ”¹ CitiBank-themed frontend UI

The goal is to create a secure, fully local RAG pipeline suitable for enterprise-style deployments.

â¸»

ğŸ—ï¸ Architecture

ğŸ”¹ Backend
	â€¢	Framework: FastAPI
	â€¢	Server: Uvicorn
	â€¢	LLM: Ollama (gemma3:latest)
	â€¢	Vector Database: ChromaDB
	â€¢	Embeddings: Open-source embedding model
	â€¢	Memory & History Storage: Redis

ğŸ”¹ Frontend
	â€¢	HTML
	â€¢	CSS
	â€¢	JavaScript
	â€¢	Bootstrap
	â€¢	Vite
	â€¢	CitiBank branding theme and logo

â¸»

ğŸ” Core Features

1ï¸âƒ£ Retrieval-Augmented Generation (RAG)
	â€¢	Upload PDF / text documents
	â€¢	Ingest documents from URLs
	â€¢	Chunking + embedding
	â€¢	Store vectors in ChromaDB
	â€¢	Retrieve relevant context during query

2ï¸âƒ£ Local LLM Inference
	â€¢	Powered by Ollama
	â€¢	Uses gemma3:latest
	â€¢	Fully offline model execution

3ï¸âƒ£ Persistent Chat Memory
	â€¢	Chat conversations stored in Redis
	â€¢	Maintains session-based context
	â€¢	Enables multi-turn conversations

4ï¸âƒ£ Prompt History
	â€¢	All prompts stored in Redis
	â€¢	Displayed in left-side navigation panel
	â€¢	Click to reload previous conversations

5ï¸âƒ£ Document Management UI
	â€¢	Uploaded documents displayed in UI
	â€¢	URL-ingested documents listed
	â€¢	Allows users to track active knowledge base

â¸»

ğŸ§  RAG Pipeline Flow
	1.	Upload document or provide URL
	2.	Extract & clean content
	3.	Chunk text
	4.	Generate embeddings
	5.	Store vectors in ChromaDB
	6.	User sends query
	7.	Retrieve relevant chunks
	8.	Send context + query to LLM
	9.	Generate final response

â¸»

ğŸ—‚ï¸ Project Structure (Example)

/backend
    main.py
    rag_pipeline.py
    redis_memory.py
    chroma_store.py

/frontend
    index.html
    src/
    assets/
    vite.config.js


â¸»

âš™ï¸ Requirements

Backend
	â€¢	Python 3.10+
	â€¢	FastAPI
	â€¢	Uvicorn
	â€¢	Redis
	â€¢	ChromaDB
	â€¢	Ollama
	â€¢	Open-source embedding model

Frontend
	â€¢	Node.js
	â€¢	Vite
	â€¢	Bootstrap

â¸»

ğŸ› ï¸ Running the Project

1ï¸âƒ£ Start Ollama

ollama run gemma3:latest

2ï¸âƒ£ Start Redis

redis-server

3ï¸âƒ£ Run Backend

uvicorn main:app --reload

4ï¸âƒ£ Run Frontend

npm install
npm run dev


â¸»

ğŸ” Design Goals
	â€¢	Fully local LLM execution
	â€¢	Enterprise-style UI (Citi theme)
	â€¢	Modular backend design
	â€¢	Scalable vector storage
	â€¢	Persistent memory architecture
	â€¢	Clean separation of frontend and backend

â¸»

ğŸ“Œ Future Enhancements
	â€¢	Multi-user authentication
	â€¢	Role-based access control
	â€¢	Document tagging
	â€¢	Streaming responses
	â€¢	Model selection dropdown
	â€¢	Vector DB filtering
	â€¢	Dockerized deployment

â¸»

ğŸ“„ License

MIT License (or specify your license)

â¸»

If you want, I can also:
	â€¢	Convert this into a clean GitHub-ready markdown with badges
	â€¢	Create a professional enterprise-style README
	â€¢	Add architecture diagrams (Mermaid)
	â€¢	Generate a Docker Compose setup section
	â€¢	Create a deployment guide for production
